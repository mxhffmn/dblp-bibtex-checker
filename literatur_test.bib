@Article{Cebiric2018,
  author    = {{\v{S}}ejla {\v{C}}ebiri{\'{c}} and Fran{\c{c}}ois Goasdou{\'{e}} and Haridimos Kondylakis and Dimitris Kotzinos and Ioana Manolescu and Georgia Troullinou and Mussab Zneika},
  journal   = {The {VLDB} Journal},
  title     = {Summarizing semantic graphs: a survey},
  year      = {2018},
  month     = {dec},
  number    = {3},
  pages     = {295--327},
  volume    = {28},
  doi       = {10.1007/s00778-018-0528-3},
  keywords  = {Semantic Graphs},
  publisher = {Springer Science and Business Media {LLC}},
}

@InCollection{Zeyen2020,
  author    = {Christian Zeyen and Ralph Bergmann},
  booktitle = {Case-Based Reasoning Research and Development},
  publisher = {Springer International Publishing},
  title     = {A*-Based Similarity Assessment of Semantic Graphs},
  year      = {2020},
  pages     = {17--32},
  doi       = {10.1007/978-3-030-58342-2_2},
  keywords  = {Semantic Graphs, Similarity, CBR},
}

@Article{Bergmann2014,
  author    = {Ralph Bergmann and Yolanda Gil},
  journal   = {Information Systems},
  title     = {Similarity assessment and efficient retrieval of semantic workflows},
  year      = {2014},
  month     = {mar},
  pages     = {115--127},
  volume    = {40},
  doi       = {10.1016/j.is.2012.07.005},
  keywords  = {CBR, Similarity},
  publisher = {Elsevier {BV}},
}

@InCollection{Hoffmann2020,
  author    = {Maximilian Hoffmann and Lukas Malburg and Patrick Klein and Ralph Bergmann},
  booktitle = {Case-Based Reasoning Research and Development},
  publisher = {Springer International Publishing},
  title     = {Using Siamese Graph Neural Networks for Similarity-Based Retrieval in Process-Oriented Case-Based Reasoning},
  year      = {2020},
  pages     = {229--244},
  doi       = {10.1007/978-3-030-58342-2_15},
  keywords  = {Siamese NN, Supervised Learning, GNN, Similarity, POCBR, CBR},
}

@Misc{Li2019,
  author    = {Li, Yujia and Gu, Chenjie and Dullien, Thomas and Vinyals, Oriol and Kohli, Pushmeet},
  title     = {Graph Matching Networks for Learning the Similarity of Graph Structured Objects},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1904.12787},
  keywords  = {GMN, Similarity, Semantic Graphs, Supervised Learning},
  publisher = {arXiv},
}

@Article{Cai2017,
  author        = {Hongyun Cai and Vincent W. Zheng and Kevin Chen-Chuan Chang},
  title         = {A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications},
  year          = {2017},
  month         = sep,
  abstract      = {Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximally preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work address these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques and application scenarios.},
  archiveprefix = {arXiv},
  eprint        = {1709.07604},
  file          = {:http\://arxiv.org/pdf/1709.07604v3:PDF},
  keywords      = {Graph Embedding, Introduction},
  primaryclass  = {cs.AI},
}

@Article{Harshvardhan2020,
  author    = {Harshvardhan GM and Mahendra Kumar Gourisaria and Manjusha Pandey and Siddharth Swarup Rautaray},
  journal   = {Computer Science Review},
  title     = {A comprehensive survey and analysis of generative models in machine learning},
  year      = {2020},
  month     = {nov},
  pages     = {100285},
  volume    = {38},
  doi       = {10.1016/j.cosrev.2020.100285},
  keywords  = {Introduction, Generative Model},
  publisher = {Elsevier {BV}},
}

@Misc{Dhariwal2021,
  author    = {Dhariwal, Prafulla and Nichol, Alex},
  title     = {Diffusion Models Beat GANs on Image Synthesis},
  year      = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2105.05233},
  keywords  = {GAN, Diffusion Model, Generative Model, Supervised Learning},
  publisher = {arXiv},
}

@Misc{Bergmann2019,
  author   = {Bergmann, Ralph and Grumbach, Lisa and Malburg, Lukas and Zeyen, Christian},
  title    = {ProCAKE: A Process-Oriented Case-Based Reasoning Framework},
  year     = {2019},
  date     = {2019},
  keywords = {CBR, POCBR},
  url      = {http://www.wi2.uni-trier.de/shared/publications/2019_BergmannGrumbachMalburgZeyen_ICCBR_Demo.pdf},
}

@Article{Bergmann2009,
  author   = {Ralph Bergmann and Klaus-Dieter Althoff and Mirjam Minor and Meike Reichle and Kerstin Bach},
  title    = {Case-Based Reasoning - Introduction and Recent Developments.},
  year     = {2009},
  keywords = {CBR, Introduction},
  url      = {http://www.wi2.uni-trier.de/shared/publications/2009_KI_CBR.pdf},
}

@Article{Hoffmann2022,
  author    = {Maximilian Hoffmann and Ralph Bergmann},
  journal   = {Algorithms},
  title     = {Using Graph Embedding Techniques in Process-Oriented Case-Based Reasoning},
  year      = {2022},
  month     = {jan},
  number    = {2},
  pages     = {27},
  volume    = {15},
  doi       = {10.3390/a15020027},
  keywords  = {Graph Embedding, CBR, POCBR},
  publisher = {{MDPI} {AG}},
}

@Article{Aamodt1994,
  author    = {Agnar Aamodt and Enric Plaza},
  journal   = {{AI} Communications},
  title     = {Case-Based Reasoning: Foundational Issues, Methodological Variations, and System Approaches},
  year      = {1994},
  number    = {1},
  pages     = {39--59},
  volume    = {7},
  doi       = {10.3233/aic-1994-7104},
  keywords  = {CBR, Introduction},
  publisher = {{IOS} Press},
}

@InCollection{Zeyen2017,
  author    = {Christian Zeyen and Gilbert MÃ¼ller and Ralph Bergmann},
  booktitle = {Case-Based Reasoning Research and Development},
  publisher = {Springer International Publishing},
  title     = {Conversational Process-Oriented Case-Based Reasoning},
  year      = {2017},
  pages     = {403--419},
  doi       = {10.1007/978-3-319-61030-6_28},
  keywords  = {CBR, POCBR},
}

@Article{Minor2014,
  author    = {Mirjam Minor and Stefania Montani and Juan A. Recio-Garc{\'{\i}}a},
  journal   = {Information Systems},
  title     = {Process-oriented case-based reasoning},
  year      = {2014},
  month     = {mar},
  pages     = {103--105},
  volume    = {40},
  doi       = {10.1016/j.is.2013.06.004},
  keywords  = {CBR, POCBR},
  publisher = {Elsevier {BV}},
}

@Article{Reijers2021,
  author    = {Hajo A. Reijers},
  journal   = {Computers in Industry},
  title     = {Business Process Management: The evolution of a discipline},
  year      = {2021},
  month     = {apr},
  pages     = {103404},
  volume    = {126},
  doi       = {10.1016/j.compind.2021.103404},
  keywords  = {BPM, Introduction},
  publisher = {Elsevier {BV}},
}

@InCollection{Malburg2020,
  author    = {Lukas Malburg and Ronny Seiger and Ralph Bergmann and Barbara Weber},
  booktitle = {Business Process Management Workshops},
  publisher = {Springer International Publishing},
  title     = {Using Physical Factory Simulation Models for Business Process Management Research},
  year      = {2020},
  pages     = {95--107},
  doi       = {10.1007/978-3-030-66498-5_8},
  keywords  = {BPM, CBR, POCBR},
}

@Article{Ontanon2020,
  author    = {Santiago Onta{\~{n}}{\'{o}}n},
  journal   = {Artificial Intelligence Review},
  title     = {An overview of distance and similarity functions for structured data},
  year      = {2020},
  month     = {feb},
  number    = {7},
  pages     = {5309--5351},
  volume    = {53},
  doi       = {10.1007/s10462-020-09821-w},
  keywords  = {Similarity, Introduction},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Babai2019,
  author    = {L{\'{a}}szl{\'{o}} Babai},
  booktitle = {Proceedings of the International Congress of Mathematicians ({ICM} 2018)},
  title     = {{Group}, {graphs}, {algorithms}: {The} {Graph} {Isomorphism} {Problem}},
  year      = {2019},
  month     = {may},
  publisher = {{WORLD} {SCIENTIFIC}},
  doi       = {10.1142/9789813272880_0183},
  keywords  = {Math, Introduction},
}

@Article{Richter2007,
  author   = {Michael M. Richter},
  journal  = {Proceedings of the Twentieth International Florida Artificial Intelligence Research Society Conference},
  title    = {Foundations of Similarity and Utility.},
  year     = {2007},
  keywords = {Math, Similarity, Introduction},
}

@Article{Sahito2021,
  author        = {Attaullah Sahito and Eibe Frank and Bernhard Pfahringer},
  journal       = {In AI 2019: Advances in Artificial Intelligence. AI 2019 . Lecture Notes in Computer Science, vol 11919. Springer, Cham},
  title         = {Semi-Supervised Learning using Siamese Networks},
  year          = {2021},
  month         = sep,
  abstract      = {Neural networks have been successfully used as classification models yielding state-of-the-art results when trained on a large number of labeled samples. These models, however, are more difficult to train successfully for semi-supervised problems where small amounts of labeled instances are available along with a large number of unlabeled instances. This work explores a new training method for semi-supervised learning that is based on similarity function learning using a Siamese network to obtain a suitable embedding. The learned representations are discriminative in Euclidean space, and hence can be used for labeling unlabeled instances using a nearest-neighbor classifier. Confident predictions of unlabeled instances are used as true labels for retraining the Siamese network on the expanded training set. This process is applied iteratively. We perform an empirical study of this iterative self-training algorithm. For improving unlabeled predictions, local learning with global consistency [22] is also evaluated.},
  archiveprefix = {arXiv},
  doi           = {10.1007/978-3-030-35288-2_47},
  eprint        = {2109.00794},
  file          = {:http\://arxiv.org/pdf/2109.00794v2:PDF},
  keywords      = {Semi-Supervised Learning, Siamese NN, Triplet Loss},
  primaryclass  = {cs.LG},
}

@Article{Kingma2014,
  author        = {Diederik P. Kingma and Danilo J. Rezende and Shakir Mohamed and Max Welling},
  title         = {Semi-Supervised Learning with Deep Generative Models},
  year          = {2014},
  month         = jun,
  abstract      = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
  archiveprefix = {arXiv},
  eprint        = {1406.5298},
  file          = {:http\://arxiv.org/pdf/1406.5298v2:PDF},
  keywords      = {Generative Model, Semi-Supervised Learning},
  primaryclass  = {cs.LG},
}

@Article{Kipf2016,
  author        = {Thomas N. Kipf and Max Welling},
  title         = {Variational Graph Auto-Encoders},
  year          = {2016},
  month         = nov,
  abstract      = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
  archiveprefix = {arXiv},
  eprint        = {1611.07308},
  file          = {:http\://arxiv.org/pdf/1611.07308v1:PDF},
  keywords      = {VGAE, VAE, GCN, Link Prediction, Unsupervised Learning, Autoencoder},
  primaryclass  = {stat.ML},
}

@Article{Struckmeier2020,
  author        = {Oliver Struckmeier and Kshitij Tiwari and Ville Kyrki},
  title         = {Autoencoding Slow Representations for Semi-supervised Data Efficient Regression},
  year          = {2020},
  month         = dec,
  abstract      = {The slowness principle is a concept inspired by the visual cortex of the brain. It postulates that the underlying generative factors of a quickly varying sensory signal change on a slower time scale. Unsupervised learning of intermediate representations utilizing abundant unlabeled sensory data can be leveraged to perform data-efficient supervised downstream regression. In this paper, we propose a general formulation of slowness for unsupervised representation learning adding a slowness regularization term to the estimate lower bound of the beta-VAE to encourage temporal similarity in observation and latent space. Within this framework we compare existing slowness regularization terms such as the L1 and L2 loss used in existing end-to-end methods, the SlowVAE and propose a new term based on Brownian motion. We empirically evaluate these slowness regularization terms with respect to their downstream task performance and data efficiency. We find that slow representations lead to equal or better downstream task performance and data efficiency in different experiment domains when compared to representations without slowness regularization. Finally, we discuss how the Frechet Inception Distance (FID), traditionally used to determine the generative capabilities of GANs, can serve as a measure to predict the performance of pre-trained Autoencoder model in a supervised downstream task and accelerate hyperparameter search.},
  archiveprefix = {arXiv},
  eprint        = {2012.06279},
  file          = {:http\://arxiv.org/pdf/2012.06279v2:PDF},
  keywords      = {Autoencoder, VAE, GVAE, Unsupervised Learning},
  primaryclass  = {cs.LG},
}

@Article{Wetzel2020,
  author        = {Sebastian J. Wetzel and Kevin Ryczko and Roger G. Melko and Isaac Tamblyn},
  title         = {Twin Neural Network Regression},
  year          = {2020},
  month         = dec,
  abstract      = {We introduce twin neural network (TNN) regression. This method predicts differences between the target values of two different data points rather than the targets themselves. The solution of a traditional regression problem is then obtained by averaging over an ensemble of all predicted differences between the targets of an unseen data point and all training data points. Whereas ensembles are normally costly to produce, TNN regression intrinsically creates an ensemble of predictions of twice the size of the training set while only training a single neural network. Since ensembles have been shown to be more accurate than single models this property naturally transfers to TNN regression. We show that TNNs are able to compete or yield more accurate predictions for different data sets, compared to other state-of-the-art methods. Furthermore, TNN regression is constrained by self-consistency conditions. We find that the violation of these conditions provides an estimate for the prediction uncertainty.},
  archiveprefix = {arXiv},
  eprint        = {2012.14873},
  file          = {:http\://arxiv.org/pdf/2012.14873v1:PDF},
  keywords      = {Siamese NN, Regression, Supervised Learning},
  primaryclass  = {cs.LG},
}

@Article{Frey2022,
  author        = {Christian M. M. Frey and Matthias Schubert},
  title         = {V-Coder: Adaptive AutoEncoder for Semantic Disclosure in Knowledge Graphs},
  year          = {2022},
  month         = jul,
  abstract      = {Semantic Web or Knowledge Graphs (KG) emerged to one of the most important information source for intelligent systems requiring access to structured knowledge. One of the major challenges is the extraction and processing of unambiguous information from textual data. Following the human perception, overlapping semantic linkages between two named entities become clear due to our common-sense about the context a relationship lives in which is not the case when we look at it from an automatically driven process of a machine. In this work, we are interested in the problem of Relational Resolution within the scope of KGs, i.e, we are investigating the inherent semantic of relationships between entities within a network. We propose a new adaptive AutoEncoder, called V-Coder, to identify relations inherently connecting entities from different domains. Those relations can be considered as being ambiguous and are candidates for disentanglement. Likewise to the Adaptive Learning Theory (ART), our model learns new patterns from the KG by increasing units in a competitive layer without discarding the previous observed patterns whilst learning the quality of each relation separately. The evaluation on real-world datasets of Freebase, Yago and NELL shows that the V-Coder is not only able to recover links from corrupted input data, but also shows that the semantic disclosure of relations in a KG show the tendency to improve link prediction. A semantic evaluation wraps the evaluation up.},
  archiveprefix = {arXiv},
  eprint        = {2208.01735},
  file          = {:http\://arxiv.org/pdf/2208.01735v1:PDF},
  keywords      = {Autoencoder, GAE, Unsupervised Learning},
  primaryclass  = {cs.AI},
}

@InProceedings{Yang2021,
  author    = {Jinfa Yang and Yongjie Shi and Xin Tong and Robin Wang and Taiyan Chen and Xianghua Ying},
  booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2021},
  title     = {Improving Knowledge Graph Embedding Using Affine Transformations of Entities Corresponding to Each Relation},
  year      = {2021},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2021.findings-emnlp.46},
  keywords  = {Semantic Graphs, Affine Transformation},
}

@InCollection{Dong2018,
  author    = {Xingping Dong and Jianbing Shen},
  booktitle = {Computer Vision {\textendash} {ECCV} 2018},
  publisher = {Springer International Publishing},
  title     = {Triplet Loss in Siamese Network for Object Tracking},
  year      = {2018},
  pages     = {472--488},
  doi       = {10.1007/978-3-030-01261-8_28},
  keywords  = {Siamese NN, Triplet Loss},
}

@Article{Li2018,
  author    = {Gaoyang Li and Xiaohua Wang and Xi Li and Aijun Yang and Mingzhe Rong},
  journal   = {Sensors},
  title     = {Partial Discharge Recognition with a Multi-Resolution Convolutional Neural Network},
  year      = {2018},
  month     = {oct},
  number    = {10},
  pages     = {3512},
  volume    = {18},
  doi       = {10.3390/s18103512},
  keywords  = {CNN, Supervised Learning},
  publisher = {{MDPI} {AG}},
}

@Article{Choudhary2021,
  author        = {Shivani Choudhary and Tarun Luthra and Ashima Mittal and Rajat Singh},
  title         = {A Survey of Knowledge Graph Embedding and Their Applications},
  year          = {2021},
  month         = jul,
  abstract      = {Knowledge Graph embedding provides a versatile technique for representing knowledge. These techniques can be used in a variety of applications such as completion of knowledge graph to predict missing information, recommender systems, question answering, query expansion, etc. The information embedded in Knowledge graph though being structured is challenging to consume in a real-world application. Knowledge graph embedding enables the real-world application to consume information to improve performance. Knowledge graph embedding is an active research area. Most of the embedding methods focus on structure-based information. Recent research has extended the boundary to include text-based information and image-based information in entity embedding. Efforts have been made to enhance the representation with context information. This paper introduces growth in the field of KG embedding from simple translation-based models to enrichment-based models. This paper includes the utility of the Knowledge graph in real-world applications.},
  archiveprefix = {arXiv},
  eprint        = {2107.07842},
  file          = {:http\://arxiv.org/pdf/2107.07842v1:PDF},
  keywords      = {Semantic Graphs, Introduction, Graph Embedding},
  primaryclass  = {cs.IR},
}

@Article{AlRfou2019,
  author        = {Rami Al-Rfou and Dustin Zelle and Bryan Perozzi},
  journal       = {Proceedings of the 2019 World Wide Web Conference (WWW '19), May 13--17, 2019, San Francisco, CA, USA},
  title         = {DDGK: Learning Graph Representations for Deep Divergence Graph Kernels},
  year          = {2019},
  month         = apr,
  abstract      = {Can neural networks learn to compare graphs without feature engineering? In this paper, we show that it is possible to learn representations for graph similarity with neither domain knowledge nor supervision (i.e.\ feature engineering or labeled graphs). We propose Deep Divergence Graph Kernels, an unsupervised method for learning representations over graphs that encodes a relaxed notion of graph isomorphism. Our method consists of three parts. First, we learn an encoder for each anchor graph to capture its structure. Second, for each pair of graphs, we train a cross-graph attention network which uses the node representations of an anchor graph to reconstruct another graph. This approach, which we call isomorphism attention, captures how well the representations of one graph can encode another. We use the attention-augmented encoder's predictions to define a divergence score for each pair of graphs. Finally, we construct an embedding space for all graphs using these pair-wise divergence scores. Unlike previous work, much of which relies on 1) supervision, 2) domain specific knowledge (e.g. a reliance on Weisfeiler-Lehman kernels), and 3) known node alignment, our unsupervised method jointly learns node representations, graph representations, and an attention-based alignment between graphs. Our experimental results show that Deep Divergence Graph Kernels can learn an unsupervised alignment between graphs, and that the learned representations achieve competitive results when used as features on a number of challenging graph classification tasks. Furthermore, we illustrate how the learned attention allows insight into the the alignment of sub-structures across graphs.},
  archiveprefix = {arXiv},
  doi           = {10.1145/3308558.3313668},
  eprint        = {1904.09671},
  file          = {:http\://arxiv.org/pdf/1904.09671v1:PDF},
  keywords      = {Graph Embedding, Semantic Graphs, Unsupervised Learning, Representation Learning, Similarity, CBR},
  primaryclass  = {cs.LG},
}

@Book{Hamilton2020,
  author    = {William L. Hamilton},
  publisher = {Springer International Publishing},
  title     = {Graph Representation Learning},
  year      = {2020},
  doi       = {10.1007/978-3-031-01588-5},
  keywords  = {Graph Embedding, Representation Learning, Introduction, Book},
}

@Book{Sun2022,
  author    = {Sun, Maosong and Shi, Chuan and Tu, Cunchao and Liu, Zhiyuan and Yang, Cheng},
  publisher = {Springer International Publishing},
  title     = {Network Embedding},
  year      = {2022},
  month     = may,
  ean       = {9783031015908},
  keywords  = {Graph Embedding, Introduction, Book},
  pagetotal = {220},
  url       = {https://www.ebook.de/de/product/42741143/maosong_sun_chuan_shi_cunchao_tu_zhiyuan_liu_cheng_yang_network_embedding.html},
}

@Article{Schroff2015,
  author        = {Florian Schroff and Dmitry Kalenichenko and James Philbin},
  title         = {FaceNet: A Unified Embedding for Face Recognition and Clustering},
  year          = {2015},
  month         = mar,
  abstract      = {Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result by 30% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.},
  archiveprefix = {arXiv},
  doi           = {10.1109/CVPR.2015.7298682},
  eprint        = {1503.03832},
  file          = {:http\://arxiv.org/pdf/1503.03832v3:PDF},
  keywords      = {cs.CV, Siamese NN, Triplet Loss},
  primaryclass  = {cs.CV},
}

@Article{Krivosheev2020,
  author        = {Evgeny Krivosheev and Mattia Atzeni and Katsiaryna Mirylenka and Paolo Scotton and Fabio Casati},
  title         = {Siamese Graph Neural Networks for Data Integration},
  year          = {2020},
  month         = jan,
  abstract      = {Data integration has been studied extensively for decades and approached from different angles. However, this domain still remains largely rule-driven and lacks universal automation. Recent development in machine learning and in particular deep learning has opened the way to more general and more efficient solutions to data integration problems. In this work, we propose a general approach to modeling and integrating entities from structured data, such as relational databases, as well as unstructured sources, such as free text from news articles. Our approach is designed to explicitly model and leverage relations between entities, thereby using all available information and preserving as much context as possible. This is achieved by combining siamese and graph neural networks to propagate information between connected entities and support high scalability. We evaluate our method on the task of integrating data about business entities, and we demonstrate that it outperforms standard rule-based systems, as well as other deep learning approaches that do not use graph-based representations.},
  archiveprefix = {arXiv},
  eprint        = {2001.06543},
  file          = {:http\://arxiv.org/pdf/2001.06543v1:PDF},
  keywords      = {cs.DB, cs.LG, Siamese NN},
  primaryclass  = {cs.DB},
}

@InProceedings{Zhou2003,
  author    = {Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas and Weston, Jason and Sch\"{o}lkopf, Bernhard},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Learning with Local and Global Consistency},
  year      = {2003},
  editor    = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
  publisher = {MIT Press},
  volume    = {16},
  keywords  = {Semi-Supervised Learning},
}

@Article{Chen2020,
  author        = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
  title         = {A Simple Framework for Contrastive Learning of Visual Representations},
  year          = {2020},
  month         = feb,
  abstract      = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archiveprefix = {arXiv},
  eprint        = {2002.05709},
  file          = {:http\://arxiv.org/pdf/2002.05709v3:PDF},
  keywords      = {cs.LG, cs.CV, stat.ML, Triplet Loss, Unsupervised Learning},
  primaryclass  = {cs.LG},
}

@Article{Chen2020a,
  author        = {Xinlei Chen and Haoqi Fan and Ross Girshick and Kaiming He},
  title         = {Improved Baselines with Momentum Contrastive Learning},
  year          = {2020},
  month         = mar,
  abstract      = {Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.},
  archiveprefix = {arXiv},
  eprint        = {2003.04297},
  file          = {:http\://arxiv.org/pdf/2003.04297v1:PDF},
  keywords      = {cs.CV, Triplet Loss, Unsupervised Learning},
  primaryclass  = {cs.CV},
}

@Article{Yu2019,
  author        = {Hong-Xing Yu and Wei-Shi Zheng and Ancong Wu and Xiaowei Guo and Shaogang Gong and Jian-Huang Lai},
  title         = {Unsupervised Person Re-identification by Soft Multilabel Learning},
  year          = {2019},
  month         = mar,
  abstract      = {Although unsupervised person re-identification (RE-ID) has drawn increasing research attentions due to its potential to address the scalability problem of supervised RE-ID models, it is very challenging to learn discriminative information in the absence of pairwise labels across disjoint camera views. To overcome this problem, we propose a deep model for the soft multilabel learning for unsupervised RE-ID. The idea is to learn a soft multilabel (real-valued label likelihood vector) for each unlabeled person by comparing (and representing) the unlabeled person with a set of known reference persons from an auxiliary domain. We propose the soft multilabel-guided hard negative mining to learn a discriminative embedding for the unlabeled target domain by exploring the similarity consistency of the visual features and the soft multilabels of unlabeled target pairs. Since most target pairs are cross-view pairs, we develop the cross-view consistent soft multilabel learning to achieve the learning goal that the soft multilabels are consistently good across different camera views. To enable effecient soft multilabel learning, we introduce the reference agent learning to represent each reference person by a reference agent in a joint embedding. We evaluate our unified deep model on Market-1501 and DukeMTMC-reID. Our model outperforms the state-of-the-art unsupervised RE-ID methods by clear margins. Code is available at https://github.com/KovenYu/MAR.},
  archiveprefix = {arXiv},
  eprint        = {1903.06325},
  file          = {:http\://arxiv.org/pdf/1903.06325v2:PDF},
  keywords      = {cs.CV, Unsupervised Learning},
  primaryclass  = {cs.CV},
}

@Article{Zhong2019,
  author        = {Zhun Zhong and Liang Zheng and Zhiming Luo and Shaozi Li and Yi Yang},
  title         = {Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identification},
  year          = {2019},
  month         = apr,
  abstract      = {This paper considers the domain adaptive person re-identification (re-ID) problem: learning a re-ID model from a labeled source domain and an unlabeled target domain. Conventional methods are mainly to reduce feature distribution gap between the source and target domains. However, these studies largely neglect the intra-domain variations in the target domain, which contain critical factors influencing the testing performance on the target domain. In this work, we comprehensively investigate into the intra-domain variations of the target domain and propose to generalize the re-ID model w.r.t three types of the underlying invariance, i.e., exemplar-invariance, camera-invariance and neighborhood-invariance. To achieve this goal, an exemplar memory is introduced to store features of the target domain and accommodate the three invariance properties. The memory allows us to enforce the invariance constraints over global training batch without significantly increasing computation cost. Experiment demonstrates that the three invariance properties and the proposed memory are indispensable towards an effective domain adaptation system. Results on three re-ID domains show that our domain adaptation accuracy outperforms the state of the art by a large margin. Code is available at: https://github.com/zhunzhong07/ECN},
  archiveprefix = {arXiv},
  eprint        = {1904.01990},
  file          = {:http\://arxiv.org/pdf/1904.01990v1:PDF},
  keywords      = {cs.CV, cs.LG, Unsupervised Learning},
  primaryclass  = {cs.CV},
}

@Article{Liu2021,
  author        = {Xiaobin Liu and Shiliang Zhang},
  title         = {Graph Consistency Based Mean-Teaching for Unsupervised Domain Adaptive Person Re-Identification},
  year          = {2021},
  month         = may,
  abstract      = {Recent works show that mean-teaching is an effective framework for unsupervised domain adaptive person re-identification. However, existing methods perform contrastive learning on selected samples between teacher and student networks, which is sensitive to noises in pseudo labels and neglects the relationship among most samples. Moreover, these methods are not effective in cooperation of different teacher networks. To handle these issues, this paper proposes a Graph Consistency based Mean-Teaching (GCMT) method with constructing the Graph Consistency Constraint (GCC) between teacher and student networks. Specifically, given unlabeled training images, we apply teacher networks to extract corresponding features and further construct a teacher graph for each teacher network to describe the similarity relationships among training images. To boost the representation learning, different teacher graphs are fused to provide the supervise signal for optimizing student networks. GCMT fuses similarity relationships predicted by different teacher networks as supervision and effectively optimizes student networks with more sample relationships involved. Experiments on three datasets, i.e., Market-1501, DukeMTMCreID, and MSMT17, show that proposed GCMT outperforms state-of-the-art methods by clear margin. Specially, GCMT even outperforms the previous method that uses a deeper backbone. Experimental results also show that GCMT can effectively boost the performance with multiple teacher and student networks. Our code is available at https://github.com/liu-xb/GCMT .},
  archiveprefix = {arXiv},
  eprint        = {2105.04776},
  file          = {:http\://arxiv.org/pdf/2105.04776v5:PDF},
  keywords      = {cs.CV, cs.AI, Unsupervised Learning},
  primaryclass  = {cs.CV},
}

@Article{Zhang2020,
  author        = {Rui Zhang and Yunxing Zhang and Xuelong Li},
  title         = {Graph Convolutional Auto-encoder with Bi-decoder and Adaptive-sharing Adjacency},
  year          = {2020},
  month         = mar,
  abstract      = {Graph autoencoder (GAE) serves as an effective unsupervised learning framework to represent graph data in a latent space for network embedding. Most exiting approaches typically focus on minimizing the reconstruction loss of graph structure but neglect the reconstruction of node features, which may result in overfitting due to the capacity of the autoencoders. Additionally, the adjacency matrix in these methods is always fixed such that the adjacency matrix cannot properly represent the connections among nodes in latent space. To solve this problem, in this paper, we propose a novel Graph Convolutional Auto-encoder with Bidecoder and Adaptive-sharing Adjacency method, namely BAGA. The framework encodes the topological structure and node features into latent representations, on which a bi-decoder is trained to reconstruct the graph structure and node features simultaneously. Furthermore, the adjacency matrix can be adaptively updated by the learned latent representations for better representing the connections among nodes in latent space. Experimental results on datasets validate the superiority of our method to the state-of-the-art network embedding methods on the clustering task.},
  archiveprefix = {arXiv},
  eprint        = {2003.04508},
  file          = {:http\://arxiv.org/pdf/2003.04508v1:PDF},
  keywords      = {cs.LG, stat.ML, Autoencoder, Unsupervised Learning},
  primaryclass  = {cs.LG},
}

@Article{Li2020,
  author        = {Jia Li and Tomasyu Yu Jiajin Li and Honglei Zhang and Kangfei Zhao and YU Rong and Hong Cheng},
  title         = {Dirichlet Graph Variational Autoencoder},
  year          = {2020},
  month         = oct,
  abstract      = {Graph Neural Networks (GNNs) and Variational Autoencoders (VAEs) have been widely used in modeling and generating graphs with latent factors. However, there is no clear explanation of what these latent factors are and why they perform well. In this work, we present Dirichlet Graph Variational Autoencoder (DGVAE) with graph cluster memberships as latent factors. Our study connects VAEs based graph generation and balanced graph cut, and provides a new way to understand and improve the internal mechanism of VAEs based graph generation. Specifically, we first interpret the reconstruction term of DGVAE as balanced graph cut in a principled way. Furthermore, motivated by the low pass characteristics in balanced graph cut, we propose a new variant of GNN named Heatts to encode the input graph into cluster memberships. Heatts utilizes the Taylor series for fast computation of heat kernels and has better low pass characteristics than Graph Convolutional Networks (GCN). Through experiments on graph generation and graph clustering, we demonstrate the effectiveness of our proposed framework.},
  archiveprefix = {arXiv},
  eprint        = {2010.04408},
  file          = {:http\://arxiv.org/pdf/2010.04408v1:PDF},
  keywords      = {cs.LG, Autoencoder, Unsupervised Learning},
  primaryclass  = {cs.LG},
}

@Article{Lin2018,
  author        = {Wu Lin and Nicolas Hubacher and Mohammad Emtiyaz Khan},
  journal       = {ICLR 2018},
  title         = {Variational Message Passing with Structured Inference Networks},
  year          = {2018},
  month         = mar,
  abstract      = {Recent efforts on combining deep models with probabilistic graphical models are promising in providing flexible models that are also easy to interpret. We propose a variational message-passing algorithm for variational inference in such models. We make three contributions. First, we propose structured inference networks that incorporate the structure of the graphical model in the inference network of variational auto-encoders (VAE). Second, we establish conditions under which such inference networks enable fast amortized inference similar to VAE. Finally, we derive a variational message passing algorithm to perform efficient natural-gradient inference while retaining the efficiency of the amortized inference. By simultaneously enabling structured, amortized, and natural-gradient inference for deep structured models, our method simplifies and generalizes existing methods.},
  archiveprefix = {arXiv},
  eprint        = {1803.05589},
  file          = {:http\://arxiv.org/pdf/1803.05589v2:PDF},
  keywords      = {stat.ML, Autoencoder, Unsupervised Learning},
  primaryclass  = {stat.ML},
}

@Article{Simonovsky2018,
  author        = {Martin Simonovsky and Nikos Komodakis},
  title         = {GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders},
  year          = {2018},
  month         = feb,
  abstract      = {Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of molecule generation.},
  archiveprefix = {arXiv},
  eprint        = {1802.03480},
  file          = {:http\://arxiv.org/pdf/1802.03480v1:PDF},
  keywords      = {cs.LG, cs.CV, cs.NE, Autoencoder, Unsupervised Learning},
  primaryclass  = {cs.LG},
}

@Article{Shi2019,
  author        = {Han Shi and Haozheng Fan and James T. Kwok},
  title         = {Effective Decoding in Graph Auto-Encoder using Triadic Closure},
  year          = {2019},
  month         = nov,
  abstract      = {The (variational) graph auto-encoder and its variants have been popularly used for representation learning on graph-structured data. While the encoder is often a powerful graph convolutional network, the decoder reconstructs the graph structure by only considering two nodes at a time, thus ignoring possible interactions among edges. On the other hand, structured prediction, which considers the whole graph simultaneously, is computationally expensive. In this paper, we utilize the well-known triadic closure property which is exhibited in many real-world networks. We propose the triad decoder, which considers and predicts the three edges involved in a local triad together. The triad decoder can be readily used in any graph-based auto-encoder. In particular, we incorporate this to the (variational) graph auto-encoder. Experiments on link prediction, node clustering and graph generation show that the use of triads leads to more accurate prediction, clustering and better preservation of the graph characteristics.},
  archiveprefix = {arXiv},
  eprint        = {1911.11322},
  file          = {:http\://arxiv.org/pdf/1911.11322v1:PDF},
  keywords      = {cs.LG, stat.ML, Autoencoder, Unsupervised Learning},
  primaryclass  = {cs.LG},
}

@Article{Tang2022,
  author        = {Mingyue Tang and Carl Yang and Pan Li},
  title         = {Graph Auto-Encoder Via Neighborhood Wasserstein Reconstruction},
  year          = {2022},
  month         = feb,
  abstract      = {Graph neural networks (GNNs) have drawn significant research attention recently, mostly under the setting of semi-supervised learning. When task-agnostic representations are preferred or supervision is simply unavailable, the auto-encoder framework comes in handy with a natural graph reconstruction objective for unsupervised GNN training. However, existing graph auto-encoders are designed to reconstruct the direct links, so GNNs trained in this way are only optimized towards proximity-oriented graph mining tasks, and will fall short when the topological structures matter. In this work, we revisit the graph encoding process of GNNs which essentially learns to encode the neighborhood information of each node into an embedding vector, and propose a novel graph decoder to reconstruct the entire neighborhood information regarding both proximity and structure via Neighborhood Wasserstein Reconstruction (NWR). Specifically, from the GNN embedding of each node, NWR jointly predicts its node degree and neighbor feature distribution, where the distribution prediction adopts an optimal-transport loss based on the Wasserstein distance. Extensive experiments on both synthetic and real-world network datasets show that the unsupervised node representations learned with NWR have much more advantageous in structure-oriented graph mining tasks, while also achieving competitive performance in proximity-oriented ones.},
  archiveprefix = {arXiv},
  eprint        = {2202.09025},
  file          = {:http\://arxiv.org/pdf/2202.09025v1:PDF},
  keywords      = {cs.LG, cs.SI, Autoencoder, Unsupervised Learning},
  primaryclass  = {cs.LG},
}

@Article{Hamilton2017,
  author        = {William L. Hamilton and Rex Ying and Jure Leskovec},
  title         = {Representation Learning on Graphs: Methods and Applications},
  year          = {2017},
  month         = sep,
  abstract      = {Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.},
  archiveprefix = {arXiv},
  eprint        = {1709.05584},
  file          = {:http\://arxiv.org/pdf/1709.05584v3:PDF},
  keywords      = {cs.SI, cs.LG, Autoencoder, Unsupervised Learning, Graph Embedding},
  primaryclass  = {cs.SI},
}

@Article{Kingma2013,
  author        = {Diederik P Kingma and Max Welling},
  title         = {Auto-Encoding Variational Bayes},
  year          = {2013},
  month         = dec,
  abstract      = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  eprint        = {1312.6114},
  file          = {:http\://arxiv.org/pdf/1312.6114v11:PDF},
  keywords      = {stat.ML, cs.LG, Autoencoder, Unsupervised Learning},
  primaryclass  = {stat.ML},
}

@InProceedings{Ahmed2013,
  author    = {Amr Ahmed and Nino Shervashidze and Shravan Narayanamurthy and Vanja Josifovski and Alexander J. Smola},
  booktitle = {Proceedings of the 22nd international conference on World Wide Web},
  title     = {Distributed large-scale natural graph factorization},
  year      = {2013},
  month     = {may},
  publisher = {{ACM}},
  doi       = {10.1145/2488388.2488393},
  groups    = {Shallow Embedding},
}

@InCollection{2002,
  author    = {Mikhail Belkin and Partha Niyogi},
  booktitle = {Advances in Neural Information Processing Systems 14},
  publisher = {The {MIT} Press},
  title     = {Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering},
  year      = {2002},
  doi       = {10.7551/mitpress/1120.003.0080},
  groups    = {Shallow Embedding},
}

@InProceedings{Cao2015,
  author    = {Shaosheng Cao and Wei Lu and Qiongkai Xu},
  booktitle = {Proceedings of the 24th {ACM} International on Conference on Information and Knowledge Management},
  title     = {{GraRep}},
  year      = {2015},
  month     = {oct},
  publisher = {{ACM}},
  doi       = {10.1145/2806416.2806512},
  groups    = {Shallow Embedding},
}

@Article{Grover2016,
  author        = {Aditya Grover and Jure Leskovec},
  title         = {node2vec: Scalable Feature Learning for Networks},
  year          = {2016},
  month         = jul,
  abstract      = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.},
  archiveprefix = {arXiv},
  eprint        = {1607.00653},
  file          = {:http\://arxiv.org/pdf/1607.00653v1:PDF},
  groups        = {Shallow Embedding},
  keywords      = {cs.SI, cs.LG, stat.ML},
  primaryclass  = {cs.SI},
}

@InProceedings{Ou2016,
  author    = {Mingdong Ou and Peng Cui and Jian Pei and Ziwei Zhang and Wenwu Zhu},
  booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
  title     = {Asymmetric Transitivity Preserving Graph Embedding},
  year      = {2016},
  month     = {aug},
  publisher = {{ACM}},
  doi       = {10.1145/2939672.2939751},
  groups    = {Shallow Embedding},
}

@Article{Perozzi2014,
  author        = {Bryan Perozzi and Rami Al-Rfou and Steven Skiena},
  title         = {DeepWalk: Online Learning of Social Representations},
  year          = {2014},
  month         = mar,
  abstract      = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide $F_1$ scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
  archiveprefix = {arXiv},
  doi           = {10.1145/2623330.2623732},
  eprint        = {1403.6652},
  file          = {:http\://arxiv.org/pdf/1403.6652v2:PDF},
  groups        = {Shallow Embedding},
  keywords      = {cs.SI, cs.LG, H.2.8; I.2.6; I.5.1},
  primaryclass  = {cs.SI},
}

@Article{Gilmer2017,
  author        = {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
  title         = {Neural Message Passing for Quantum Chemistry},
  year          = {2017},
  month         = apr,
  abstract      = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  archiveprefix = {arXiv},
  eprint        = {1704.01212},
  file          = {:http\://arxiv.org/pdf/1704.01212v2:PDF},
  groups        = {Neural Message Passing},
  keywords      = {cs.LG, I.2.6},
  primaryclass  = {cs.LG},
}

@Misc{Battaglia2018,
  author    = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  title     = {Relational inductive biases, deep learning, and graph networks},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1806.01261},
  groups    = {Neural Message Passing},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Elman1990,
  author    = {Jeffrey L. Elman},
  journal   = {Cognitive Science},
  title     = {Finding Structure in Time},
  year      = {1990},
  month     = {mar},
  number    = {2},
  pages     = {179--211},
  volume    = {14},
  doi       = {10.1207/s15516709cog1402_1},
  groups    = {Neural Message Passing},
  publisher = {Wiley},
}

@Article{Kipf2016a,
  author        = {Kipf, Thomas N. and Welling, Max},
  title         = {Semi-Supervised Classification with Graph Convolutional Networks},
  year          = {2016},
  month         = sep,
  abstract      = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1609.02907},
  eprint        = {1609.02907},
  file          = {:http\://arxiv.org/pdf/1609.02907v4:PDF},
  keywords      = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, Semi-Supervised Learning},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Zaheer2017,
  author        = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander},
  title         = {Deep Sets},
  year          = {2017},
  month         = mar,
  abstract      = {We study the problem of designing models for machine learning tasks defined on \emph{sets}. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics \cite{poczos13aistats}, to anomaly detection in piezometer data of embankment dams \cite{Jung15Exploration}, to cosmology \cite{Ntampaka16Dynamical,Ravanbakhsh16ICML1}. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1703.06114},
  eprint        = {1703.06114},
  file          = {:http\://arxiv.org/pdf/1703.06114v3:PDF},
  groups        = {Neural Message Passing},
  keywords      = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Qi2016,
  author        = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
  title         = {PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},
  year          = {2016},
  month         = dec,
  abstract      = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1612.00593},
  eprint        = {1612.00593},
  file          = {:http\://arxiv.org/pdf/1612.00593v2:PDF},
  groups        = {Neural Message Passing},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Murphy2018,
  author        = {Murphy, Ryan L. and Srinivasan, Balasubramaniam and Rao, Vinayak and Ribeiro, Bruno},
  journal       = {ICLR 2019},
  title         = {Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},
  year          = {2018},
  month         = nov,
  abstract      = {We consider a simple and overarching representation for permutation-invariant functions of sequences (or multiset functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with $k$-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1811.01900},
  eprint        = {1811.01900},
  file          = {:http\://arxiv.org/pdf/1811.01900v3:PDF},
  groups        = {Neural Message Passing},
  keywords      = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Hochreiter1997,
  author    = {Sepp Hochreiter and JÃ¼rgen Schmidhuber},
  journal   = {Neural Computation},
  title     = {Long Short-Term Memory},
  year      = {1997},
  month     = {nov},
  number    = {8},
  pages     = {1735--1780},
  volume    = {9},
  doi       = {10.1162/neco.1997.9.8.1735},
  groups    = {Neural Message Passing},
  publisher = {{MIT} Press - Journals},
}

@Article{Rezende2014,
  author        = {Danilo Jimenez Rezende and Shakir Mohamed and Daan Wierstra},
  title         = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  year          = {2014},
  month         = jan,
  abstract      = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
  archiveprefix = {arXiv},
  eprint        = {1401.4082},
  file          = {:http\://arxiv.org/pdf/1401.4082v3:PDF},
  keywords      = {stat.ML, cs.AI, cs.LG, stat.CO, stat.ME, Autoencoder},
  primaryclass  = {stat.ML},
}

@Article{Sun2021,
  author    = {Dengdi Sun and Dashuang Li and Zhuanlian Ding and Xingyi Zhang and Jin Tang},
  journal   = {Knowledge-Based Systems},
  title     = {Dual-decoder graph autoencoder for unsupervised graph representation learning},
  year      = {2021},
  month     = {dec},
  pages     = {107564},
  volume    = {234},
  doi       = {10.1016/j.knosys.2021.107564},
  keywords  = {Autoencoder},
  publisher = {Elsevier {BV}},
}

@Article{Cui2019,
  author    = {Peng Cui and Xiao Wang and Jian Pei and Wenwu Zhu},
  journal   = {{IEEE} Transactions on Knowledge and Data Engineering},
  title     = {A Survey on Network Embedding},
  year      = {2019},
  month     = {may},
  number    = {5},
  pages     = {833--852},
  volume    = {31},
  doi       = {10.1109/tkde.2018.2849727},
  keywords  = {Graph Embedding},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Shorten2019,
  author    = {Connor Shorten and Taghi M. Khoshgoftaar},
  journal   = {Journal of Big Data},
  title     = {A survey on Image Data Augmentation for Deep Learning},
  year      = {2019},
  month     = {jul},
  number    = {1},
  volume    = {6},
  doi       = {10.1186/s40537-019-0197-0},
  groups    = {Data Augmentation},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Shorten2021,
  author    = {Connor Shorten and Taghi M. Khoshgoftaar and Borko Furht},
  journal   = {Journal of Big Data},
  title     = {Text Data Augmentation for Deep Learning},
  year      = {2021},
  month     = {jul},
  number    = {1},
  volume    = {8},
  doi       = {10.1186/s40537-021-00492-0},
  groups    = {Data Augmentation},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Dyk2001,
  author    = {David A van Dyk and Xiao-Li Meng},
  journal   = {Journal of Computational and Graphical Statistics},
  title     = {The Art of Data Augmentation},
  year      = {2001},
  month     = {mar},
  number    = {1},
  pages     = {1--50},
  volume    = {10},
  doi       = {10.1198/10618600152418584},
  groups    = {Data Augmentation},
  publisher = {Informa {UK} Limited},
}

@Misc{Bruna2013,
  author    = {Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
  title     = {Spectral Networks and Locally Connected Networks on Graphs},
  year      = {2013},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1312.6203},
  keywords  = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, Graph Embedding},
  publisher = {arXiv},
}

@Misc{Dai2016,
  author    = {Dai, Hanjun and Dai, Bo and Song, Le},
  title     = {Discriminative Embeddings of Latent Variable Models for Structured Data},
  year      = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1603.05629},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, Graph Embedding},
  publisher = {arXiv},
}

@Misc{Hamilton2017a,
  author    = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  title     = {Inductive Representation Learning on Large Graphs},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1706.02216},
  keywords  = {Social and Information Networks (cs.SI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, Graph Embedding},
  publisher = {arXiv},
}

@Article{Fukushima1980,
  author    = {Kunihiko Fukushima},
  journal   = {Biological Cybernetics},
  title     = {Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
  year      = {1980},
  month     = {apr},
  number    = {4},
  pages     = {193--202},
  volume    = {36},
  doi       = {10.1007/bf00344251},
  keywords  = {Introduction},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Gori,
  author    = {M. Gori and G. Monfardini and F. Scarselli},
  booktitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
  title     = {A new model for learning in graph domains},
  year      = {2005},
  publisher = {{IEEE}},
  doi       = {10.1109/ijcnn.2005.1555942},
  keywords  = {Introduction},
}

@Article{Li2022,
  author    = {Zewen Li and Fan Liu and Wenjie Yang and Shouheng Peng and Jun Zhou},
  journal   = {{IEEE} Transactions on Neural Networks and Learning Systems},
  title     = {A Survey of Convolutional Neural Networks: Analysis, Applications, and Prospects},
  year      = {2022},
  month     = {dec},
  number    = {12},
  pages     = {6999--7019},
  volume    = {33},
  doi       = {10.1109/tnnls.2021.3084827},
  keywords  = {Introduction},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Liao2019,
  author        = {Renjie Liao and Yujia Li and Yang Song and Shenlong Wang and Charlie Nash and William L. Hamilton and David Duvenaud and Raquel Urtasun and Richard S. Zemel},
  title         = {Efficient Graph Generation with Graph Recurrent Attention Networks},
  year          = {2019},
  month         = oct,
  abstract      = {We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs). Our model generates graphs one block of nodes and associated edges at a time. The block size and sampling stride allow us to trade off sample quality for efficiency. Compared to previous RNN-based graph generative models, our framework better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention. This not only reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs. Moreover, we parameterize the output distribution per block using a mixture of Bernoulli, which captures the correlations among generated edges within the block. Finally, we propose to handle node orderings in generation by marginalizing over a family of canonical orderings. On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models. Additionally, we show our model is capable of generating large graphs of up to 5K nodes with good quality. To the best of our knowledge, GRAN is the first deep graph generative model that can scale to this size. Our code is released at: https://github.com/lrjconan/GRAN.},
  archiveprefix = {arXiv},
  eprint        = {1910.00760},
  file          = {:http\://arxiv.org/pdf/1910.00760v3:PDF},
  groups        = {Data Augmentation},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{You2018,
  author        = {Jiaxuan You and Rex Ying and Xiang Ren and William L. Hamilton and Jure Leskovec},
  title         = {GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models},
  year          = {2018},
  month         = feb,
  abstract      = {Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far. In order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.},
  archiveprefix = {arXiv},
  eprint        = {1802.08773},
  file          = {:http\://arxiv.org/pdf/1802.08773v3:PDF},
  groups        = {Data Augmentation},
  keywords      = {cs.LG, cs.AI, cs.SI, I.2.6},
  primaryclass  = {cs.LG},
}

@InCollection{Chicco2020,
  author    = {Davide Chicco},
  booktitle = {Methods in Molecular Biology},
  publisher = {Springer {US}},
  title     = {Siamese Neural Networks: An Overview},
  year      = {2020},
  month     = {aug},
  pages     = {73--94},
  doi       = {10.1007/978-1-0716-0826-5_3},
  keywords  = {Siamese NN},
}

@Article{Pan2022,
  author    = {Shaojun Pan and Chengkai Zhu and Xing-Ming Zhao and Luis Pedro Coelho},
  journal   = {Nature Communications},
  title     = {A deep siamese neural network improves metagenome-assembled genomes in microbiome datasets across different environments},
  year      = {2022},
  month     = {apr},
  number    = {1},
  volume    = {13},
  doi       = {10.1038/s41467-022-29843-y},
  keywords  = {Siamese NN},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Gong2023,
  author    = {Yuxuan Gong and Yuqi Yue and Weidong Ji and Guohui Zhou},
  journal   = {Scientific Reports},
  title     = {Cross-domain few-shot learning based on pseudo-Siamese neural network},
  year      = {2023},
  month     = {jan},
  number    = {1},
  volume    = {13},
  doi       = {10.1038/s41598-023-28588-y},
  keywords  = {Siamese NN},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Hadsell,
  author    = {R. Hadsell and S. Chopra and Y. LeCun},
  booktitle = {2006 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 ({CVPR}{\textquotesingle}06)},
  title     = {Dimensionality Reduction by Learning an Invariant Mapping},
  publisher = {{IEEE}},
  doi       = {10.1109/cvpr.2006.100},
  keywords  = {Triplet Loss},
}

@Article{Hermans2017,
  author        = {Alexander Hermans and Lucas Beyer and Bastian Leibe},
  title         = {In Defense of the Triplet Loss for Person Re-Identification},
  year          = {2017},
  month         = mar,
  abstract      = {In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person re-identification subfield is no exception to this. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin.},
  archiveprefix = {arXiv},
  eprint        = {1703.07737},
  file          = {:http\://arxiv.org/pdf/1703.07737v4:PDF},
  keywords      = {cs.CV, cs.NE, Triplet Loss},
  primaryclass  = {cs.CV},
}

@Article{Fang2023,
  author    = {Jiansheng Fang and Ming Zeng and Xiaoqing Zhang and Hongbo Liu and Yitian Zhao and Peng Zhang and Hong Yang and Junling Liu and Hanpei Miao and Yan Hu and Jiang Liu},
  journal   = {Biomedical Signal Processing and Control},
  title     = {Deep metric learning with mirror attention and fine triplet loss for fundus image retrieval in ophthalmology},
  year      = {2023},
  month     = {feb},
  pages     = {104277},
  volume    = {80},
  doi       = {10.1016/j.bspc.2022.104277},
  keywords  = {Triplet Loss},
  publisher = {Elsevier {BV}},
}

@Article{Li2022a,
  author    = {Changshun Li and Peng Chen and Li Yao and Qiming Fu and Yu Dai and Junyan Yang},
  journal   = {Textile Research Journal},
  title     = {Clothes retrieval based on {ResNet} and cluster triplet loss},
  year      = {2022},
  month     = {dec},
  pages     = {004051752211430},
  doi       = {10.1177/00405175221143073},
  keywords  = {Triplet Loss},
  publisher = {{SAGE} Publications},
}

@Article{Boutros2022,
  author    = {Fadi Boutros and Naser Damer and Florian Kirchbuchner and Arjan Kuijper},
  journal   = {Pattern Recognition},
  title     = {Self-restrained triplet loss for accurate masked face recognition},
  year      = {2022},
  month     = {apr},
  pages     = {108473},
  volume    = {124},
  doi       = {10.1016/j.patcog.2021.108473},
  keywords  = {Triplet Loss},
  publisher = {Elsevier {BV}},
}

@Article{Chen2021,
  author    = {Xiaoyu Chen and Henry Y. K. Lau},
  journal   = {Applied Intelligence},
  title     = {The identity-level angular triplet loss for cross-age face recognition},
  year      = {2021},
  month     = {sep},
  number    = {6},
  pages     = {6330--6339},
  volume    = {52},
  doi       = {10.1007/s10489-021-02742-3},
  keywords  = {Triplet Loss},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Tang2022a,
  author    = {Zengming Tang and Jun Huang},
  journal   = {{ACM} Transactions on Multimedia Computing, Communications, and Applications},
  title     = {Harmonious Multi-branch Network for Person Re-identification with Harder Triplet Loss},
  year      = {2022},
  month     = {mar},
  number    = {4},
  pages     = {1--21},
  volume    = {18},
  doi       = {10.1145/3501405},
  keywords  = {Triplet Loss},
  publisher = {Association for Computing Machinery ({ACM})},
}

@Misc{Ghosh2021,
  author    = {Ghosh, Adhiraj and Shanmugalingam, Kuruparan and Lin, Wen-Yan},
  title     = {Relation Preserving Triplet Mining for Stabilising the Triplet Loss in Re-identification Systems},
  year      = {2021},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  doi       = {10.48550/ARXIV.2110.07933},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Triplet Loss},
  publisher = {arXiv},
}

@Article{Cover1967,
  author    = {T. Cover and P. Hart},
  journal   = {{IEEE} Transactions on Information Theory},
  title     = {Nearest neighbor pattern classification},
  year      = {1967},
  month     = {jan},
  number    = {1},
  pages     = {21--27},
  volume    = {13},
  doi       = {10.1109/tit.1967.1053964},
  keywords  = {Triplet Loss},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Qian2022,
  author    = {Jinchuan Qian and Zhihuan Song and Yuan Yao and Zheren Zhu and Xinmin Zhang},
  journal   = {Chemometrics and Intelligent Laboratory Systems},
  title     = {A review on autoencoder based representation learning for fault detection and diagnosis in industrial processes},
  year      = {2022},
  month     = {dec},
  pages     = {104711},
  volume    = {231},
  doi       = {10.1016/j.chemolab.2022.104711},
  keywords  = {Autoencoder},
  publisher = {Elsevier {BV}},
}

@Article{Yang2022,
  author    = {Zheng Yang and Binbin Xu and Wei Luo and Fei Chen},
  journal   = {Measurement},
  title     = {Autoencoder-based representation learning and its application in intelligent fault diagnosis: A review},
  year      = {2022},
  month     = {feb},
  pages     = {110460},
  volume    = {189},
  doi       = {10.1016/j.measurement.2021.110460},
  keywords  = {Autoencoder},
  publisher = {Elsevier {BV}},
}

@Misc{Chen2022,
  author    = {Chen, Xiaokang and Ding, Mingyu and Wang, Xiaodi and Xin, Ying and Mo, Shentong and Wang, Yunhao and Han, Shumin and Luo, Ping and Zeng, Gang and Wang, Jingdong},
  title     = {Context Autoencoder for Self-Supervised Representation Learning},
  year      = {2022},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {10.48550/ARXIV.2202.03026},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Autoencoder},
  publisher = {arXiv},
}

@Misc{Zhang2022,
  author    = {Zhang, Sixiao and Chen, Hongxu and Yang, Haoran and Sun, Xiangguo and Yu, Philip S. and Xu, Guandong},
  title     = {Graph Masked Autoencoders with Transformers},
  year      = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2202.08391},
  keywords  = {Machine Learning (cs.LG), Information Retrieval (cs.IR), FOS: Computer and information sciences, Autoencoder},
  publisher = {arXiv},
}

@InProceedings{Guo2022,
  author    = {Zhihao Guo and Feng Wang and Kaixuan Yao and Jiye Liang and Zhiqiang Wang},
  booktitle = {Proceedings of the Fifteenth {ACM} International Conference on Web Search and Data Mining},
  title     = {Multi-Scale Variational Graph {AutoEncoder} for Link Prediction},
  year      = {2022},
  month     = {feb},
  publisher = {{ACM}},
  doi       = {10.1145/3488560.3498531},
  keywords  = {Autoencoder},
}

@Article{Gan2022,
  author    = {Yanglan Gan and Xingyu Huang and Guobing Zou and Shuigeng Zhou and Jihong Guan},
  journal   = {Briefings in Bioinformatics},
  title     = {Deep structural clustering for single-cell {RNA}-seq data jointly through autoencoder and graph neural network},
  year      = {2022},
  month     = {feb},
  number    = {2},
  volume    = {23},
  doi       = {10.1093/bib/bbac018},
  keywords  = {Autoencoder},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Ying2018,
  author        = {Rex Ying and Ruining He and Kaifeng Chen and Pong Eksombatchai and William L. Hamilton and Jure Leskovec},
  title         = {Graph Convolutional Neural Networks for Web-Scale Recommender Systems},
  year          = {2018},
  month         = jun,
  abstract      = {Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.},
  archiveprefix = {arXiv},
  doi           = {10.1145/3219819.3219890},
  eprint        = {1806.01973},
  file          = {:http\://arxiv.org/pdf/1806.01973v1:PDF},
  groups        = {Neural Message Passing},
  keywords      = {cs.IR, cs.LG, stat.ML},
  primaryclass  = {cs.IR},
}

@Misc{Velickovic2017,
  author    = {VeliÄkoviÄ, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and LiÃ², Pietro and Bengio, Yoshua},
  title     = {Graph Attention Networks},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1710.10903},
  groups    = {Neural Message Passing},
  keywords  = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Social and Information Networks (cs.SI), FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Misc{Zhan2023,
  author    = {Zhan, Huixin and Zhang, Kun and Lu, Keyi and Sheng, Victor S.},
  title     = {Measuring the Privacy Leakage via Graph Reconstruction Attacks on Simplicial Neural Networks (Student Abstract)},
  year      = {2023},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {10.48550/ARXIV.2302.04373},
  groups    = {Neural Message Passing},
  keywords  = {Machine Learning (cs.LG), Cryptography and Security (cs.CR), FOS: Computer and information sciences, I.2.6, 51Hxx},
  publisher = {arXiv},
}

@Article{Xuan2021,
  author    = {Ping Xuan and Dong Wang and Hui Cui and Tiangang Zhang and Toshiya Nakaguchi},
  journal   = {Briefings in Bioinformatics},
  title     = {Integration of pairwise neighbor topologies and {miRNA} family and cluster attributes for {miRNA}{\textendash}disease association prediction},
  year      = {2021},
  month     = {oct},
  number    = {1},
  volume    = {23},
  doi       = {10.1093/bib/bbab428},
  keywords  = {Autoencoder},
  publisher = {Oxford University Press ({OUP})},
}

@Misc{Abadi2016,
  author    = {Abadi, MartÃ­n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  title     = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
  year      = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1603.04467},
  keywords  = {Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (cs.LG), FOS: Computer and information sciences, Introduction},
  publisher = {arXiv},
}

@Article{Ryen2022,
  author    = {Vetle Ryen and Ahmet Soylu and Dumitru Roman},
  journal   = {Future Internet},
  title     = {Building Semantic Knowledge Graphs from (Semi-)Structured Data: A Review},
  year      = {2022},
  month     = {apr},
  number    = {5},
  pages     = {129},
  volume    = {14},
  doi       = {10.3390/fi14050129},
  keywords  = {Semantic Graphs},
  publisher = {{MDPI} {AG}},
}

@Article{Paulheim2016,
  author    = {Heiko Paulheim},
  journal   = {Semantic Web},
  title     = {Knowledge graph refinement: A survey of approaches and evaluation methods},
  year      = {2016},
  month     = {dec},
  number    = {3},
  pages     = {489--508},
  volume    = {8},
  doi       = {10.3233/sw-160218},
  editor    = {Philipp Cimiano},
  keywords  = {Semantic Graphs},
  publisher = {{IOS} Press},
}

@Article{BernersLee2001,
  author    = {Berners-Lee, Tim and Hendler, James and Lassila, Ora},
  journal   = {Scientific American},
  title     = {The Semantic Web},
  year      = {2001},
  month     = {5},
  number    = {5},
  pages     = {34-43},
  volume    = {284},
  date      = {2001-05},
  doi       = {10.1038/scientificamerican0501-34},
  keywords  = {Semantic Graphs},
  publisher = {Springer Science and Business Media LLC},
}

@Article{W3C,
  author   = {World Wide Web Consortium (W3C)},
  title    = {World Wide Web Consortium (W3C)},
  keywords = {Semantic Graphs},
  url      = {www.w3.org/},
}

@TechReport{Lassila:99:RDF,
  author      = {Ora Lassila},
  institution = {W3C},
  title       = {Resource Description Framework ({RDF}) Model and Syntax Specification},
  year        = {1999},
  month       = feb,
  type        = {{W3C} Recommendation},
  keywords    = {Semantic Graphs},
  url         = {https://www.w3.org/TR/1999/REC-rdf-syntax-19990222/},
}

@Misc{Sowa2006,
  author    = {John F Sowa},
  month     = {jan},
  title     = {Semantic Networks},
  year      = {2006},
  doi       = {10.1002/0470018860.s00065},
  keywords  = {Semantic Graphs},
  publisher = {John Wiley {\&} Sons, Ltd},
}


@InCollection{Minor,
  author    = {Mirjam Minor and Alexander Tartakovski and Ralph Bergmann},
  booktitle = {Case-Based Reasoning Research and Development},
  publisher = {Springer Berlin Heidelberg},
  title     = {Representation and Structure-Based Similarity Assessment for Agile Workflows},
  pages     = {224--238},
  doi       = {10.1007/978-3-540-74141-1_16},
  keywords  = {CBR},
}

@InProceedings{Ge2021,
    author = {Yunhao Ge and Yunkui Pang and Linwei Li and Laurent Itti},
    booktitle = {Neural Compression: From Information Theory to Applications -- Workshop @ ICLR 2021},
    title = {Graph Autoencoder for Graph Compression and Representation Learning},
    year = {2021},
    keywords = {CBR},
    url = {https://openreview.net/forum?id=Bo2LZfaVHNi},
}

@Article{Weiss2016,
  author    = {Karl Weiss and Taghi M. Khoshgoftaar and DingDing Wang},
  journal   = {Journal of Big Data},
  title     = {A survey of transfer learning},
  year      = {2016},
  month     = {may},
  number    = {1},
  volume    = {3},
  doi       = {10.1186/s40537-016-0043-6},
  groups    = {Data Augmentation},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Goyal2018,
  author    = {Palash Goyal and Emilio Ferrara},
  journal   = {Knowledge-Based Systems},
  title     = {Graph embedding techniques, applications, and performance: A survey},
  year      = {2018},
  month     = {jul},
  pages     = {78--94},
  volume    = {151},
  doi       = {10.1016/j.knosys.2018.03.022},
  keywords  = {Graph Embedding},
  publisher = {Elsevier {BV}},
}

@Misc{Perozzi2016,
  author    = {Perozzi, Bryan and Kulkarni, Vivek and Chen, Haochen and Skiena, Steven},
  title     = {Don't Walk, Skip! Online Learning of Multi-scale Network Embeddings},
  year      = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1605.02115},
  keywords  = {Social and Information Networks (cs.SI), Physics and Society (physics.soc-ph), FOS: Computer and information sciences, FOS: Physical sciences, Graph Embedding},
  publisher = {arXiv},
}

@InCollection{Klein2019,
  author    = {Patrick Klein and Lukas Malburg and Ralph Bergmann},
  booktitle = {Case-Based Reasoning Research and Development},
  publisher = {Springer International Publishing},
  title     = {Learning Workflow Embeddings to Improve the Performance of Similarity-Based Retrieval for Process-Oriented Case-Based Reasoning},
  year      = {2019},
  pages     = {188--203},
  doi       = {10.1007/978-3-030-29249-2_13},
  keywords  = {Graph Embedding},
}

@Article{Mathisen2021,
  author    = {Bj{\o}rn Magnus Mathisen and Kerstin Bach and Agnar Aamodt},
  journal   = {Applied Intelligence},
  title     = {Using extended siamese networks to provide decision support in aquaculture operations},
  year      = {2021},
  month     = {mar},
  number    = {11},
  pages     = {8107--8118},
  volume    = {51},
  doi       = {10.1007/s10489-021-02251-3},
  keywords  = {CBR},
  publisher = {Springer Science and Business Media {LLC}},
}

@InCollection{Amin2019,
  author    = {Kareem Amin and George Lancaster and Stelios Kapetanakis and Klaus-Dieter Althoff and Andreas Dengel and Miltos Petridis},
  booktitle = {Advances in Intelligent Systems and Computing},
  publisher = {Springer International Publishing},
  title     = {Advanced Similarity Measures Using Word Embeddings and Siamese Networks in {CBR}},
  year      = {2019},
  month     = {aug},
  pages     = {449--462},
  doi       = {10.1007/978-3-030-29513-4_32},
  keywords  = {CBR},
}

@InCollection{Corchado2001,
  author    = {Juan M. Corchado and B. Lees},
  booktitle = {Soft Computing in Case Based Reasoning},
  publisher = {Springer London},
  title     = {Adaptation of Cases for Case Based Forecasting with Neural Network Support},
  year      = {2001},
  pages     = {293--319},
  doi       = {10.1007/978-1-4471-0687-6_13},
  keywords  = {CBR},
}

@InCollection{Dieterle2014,
  author    = {Sebastian Dieterle and Ralph Bergmann},
  booktitle = {Case-Based Reasoning Research and Development},
  publisher = {Springer International Publishing},
  title     = {A Hybrid {CBR}-{ANN} Approach to the Appraisal of Internet Domain Names},
  year      = {2014},
  pages     = {95--109},
  doi       = {10.1007/978-3-319-11209-1_8},
  keywords  = {CBR},
}

@Article{Mathisen2019,
  author    = {Bj{\o}rn Magnus Mathisen and Agnar Aamodt and Kerstin Bach and Helge Langseth},
  journal   = {Progress in Artificial Intelligence},
  title     = {Learning similarity measures from data},
  year      = {2019},
  month     = {oct},
  number    = {2},
  pages     = {129--143},
  volume    = {9},
  doi       = {10.1007/s13748-019-00201-2},
  keywords  = {CBR},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Liu2019,
  author    = {Yang Liu and Eunice Jun and Qisheng Li and Jeffrey Heer},
  journal   = {Computer Graphics Forum},
  title     = {Latent Space Cartography: Visual Analysis of Vector Space Embeddings},
  year      = {2019},
  month     = {jun},
  number    = {3},
  pages     = {67--78},
  volume    = {38},
  doi       = {10.1111/cgf.13672},
  keywords  = {Graph Embedding},
  publisher = {Wiley},
}

@InCollection{Champin,
  author    = {Pierre-Antoine Champin and Christine Solnon},
  booktitle = {Case-Based Reasoning Research and Development},
  publisher = {Springer Berlin Heidelberg},
  title     = {Measuring the Similarity of Labeled Graphs},
  pages     = {80--95},
  doi       = {10.1007/3-540-45006-8_9},
  keywords  = {Introduction},
}

@InCollection{Dijkman2009,
  author    = {Remco Dijkman and Marlon Dumas and Luciano Garc{\'{\i}}a-Ba{\~{n}}uelos},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  title     = {Graph Matching Algorithms for Business Process Model Similarity Search},
  year      = {2009},
  pages     = {48--63},
  doi       = {10.1007/978-3-642-03848-8_5},
  keywords  = {Introduction},
}

@InProceedings{Chopra,
  author    = {S. Chopra and R. Hadsell and Y. LeCun},
  booktitle = {2005 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition ({CVPR}{\textquotesingle}05)},
  title     = {Learning a Similarity Metric Discriminatively, with Application to Face Verification},
  publisher = {{IEEE}},
  doi       = {10.1109/cvpr.2005.202},
  keywords  = {Siamese NN},
}

@InProceedings{Berlemont2015,
  author    = {Samuel Berlemont and Gregoire Lefebvre and Stefan Duffner and Christophe Garcia},
  booktitle = {2015 11th {IEEE} International Conference and Workshops on Automatic Face and Gesture Recognition ({FG})},
  title     = {Siamese neural network based similarity metric for inertial gesture classification and rejection},
  year      = {2015},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/fg.2015.7163112},
  keywords  = {Siamese NN},
}

@InProceedings{Martin2017,
  author    = {Kyle Martin and Nirmalie Wiratunga and Sadiq Sani and Stewart Massie and J{\'e}r{\'e}mie Clos},
  booktitle = {International Conference on Case-Based Reasoning},
  title     = {A Convolutional Siamese Network for Developing Similarity Knowledge in the SelfBACK Dataset},
  year      = {2017},
  keywords  = {Siamese NN},
}

@Article{Madhusudan2004,
  author    = {Therani Madhusudan and J. Leon Zhao and Byron Marshall},
  journal   = {Data and Knowledge Engineering},
  title     = {A case-based reasoning framework for workflow model management},
  year      = {2004},
  month     = {jul},
  number    = {1},
  pages     = {87--115},
  volume    = {50},
  doi       = {10.1016/j.datak.2004.01.005},
  keywords  = {CBR},
  publisher = {Elsevier {BV}},
}

@Misc{Zhang2020a,
  author    = {Zhang, Rui and Zhang, Yunxing and Li, Xuelong},
  title     = {Unsupervised Graph Embedding via Adaptive Graph Learning},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2003.04508},
  keywords  = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, Unsupervised Learning},
  publisher = {arXiv},
}

@InProceedings{Giles1998,
  author    = {C. Lee Giles and Kurt D. Bollacker and Steve Lawrence},
  booktitle = {Proceedings of the third {ACM} conference on Digital libraries - {DL} {\textquotesingle}98},
  title     = {{CiteSeer}},
  year      = {1998},
  publisher = {{ACM} Press},
  doi       = {10.1145/276675.276685},
}

@Article{Zuo2020,
  author    = {Renguang Zuo and Ziye Wang},
  journal   = {Natural Resources Research},
  title     = {Effects of Random Negative Training Samples on Mineral Prospectivity Mapping},
  year      = {2020},
  month     = {apr},
  number    = {6},
  pages     = {3443--3455},
  volume    = {29},
  doi       = {10.1007/s11053-020-09668-6},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Tang2019,
  author    = {Xianzhe Tang and Haoyuan Hong and Yuqin Shu and Huijun Tang and Jiufeng Li and Wei Liu},
  journal   = {Journal of Hydrology},
  title     = {Urban waterlogging susceptibility assessment based on a {PSO}-{SVM} method using a novel repeatedly random sampling idea to select negative samples},
  year      = {2019},
  month     = {sep},
  pages     = {583--595},
  volume    = {576},
  doi       = {10.1016/j.jhydrol.2019.06.058},
  publisher = {Elsevier {BV}},
}

@Article{Tang2020,
  author    = {Xianzhe Tang and Takashi Machimura and Jiufeng Li and Wei Liu and Haoyuan Hong},
  journal   = {Journal of Environmental Management},
  title     = {A novel optimized repeatedly random undersampling for selecting negative samples: A case study in an {SVM}-based forest fire susceptibility assessment},
  year      = {2020},
  month     = {oct},
  pages     = {111014},
  volume    = {271},
  doi       = {10.1016/j.jenvman.2020.111014},
  publisher = {Elsevier {BV}},
}

@Misc{Wu2017,
  author    = {Wu, Ledell and Fisch, Adam and Chopra, Sumit and Adams, Keith and Bordes, Antoine and Weston, Jason},
  title     = {StarSpace: Embed All The Things!},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1709.03856},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, Graph Embedding},
  publisher = {arXiv},
}

@InProceedings{Naqvi2022,
  author    = {Syed Meesam Raza Naqvi and Mohammad Ghufran and Safa Meraghni and Christophe Varnier and Jean-Marc Nicod and Noureddine Zerhouni},
  booktitle = {2022 Prognostics and Health Management Conference ({PHM}-2022 London)},
  title     = {{CBR}-Based Decision Support System for Maintenance Text Using {NLP} for an Aviation Case Study},
  year      = {2022},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/phm2022-london52454.2022.00067},
  keywords  = {CBR},
}

@Misc{Devlin2018,
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1810.04805},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, CBR},
  publisher = {arXiv},
}

@Misc{Mikolov2013,
  author    = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  year      = {2013},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1301.3781},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, Unsupervised Learning},
  publisher = {arXiv},
}

@InProceedings{Chourib2022,
  author    = {Ikram Chourib and Gwenael Guillard and Imed Riadh Farah and Basel Solaiman},
  booktitle = {2022 6th International Conference on Advanced Technologies for Signal and Image Processing ({ATSIP})},
  title     = {Structured Case Base Knowledge using Unsupervised Learning},
  year      = {2022},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/atsip55956.2022.9805879},
  keywords  = {CBR},
}

@InCollection{Lenz2019,
  author    = {Mirko Lenz and Stefan Ollinger and Premtim Sahitaj and Ralph Bergmann},
  booktitle = {Case-Based Reasoning Research and Development},
  publisher = {Springer International Publishing},
  title     = {Semantic Textual Similarity Measures for Case-Based Retrieval of Argument Graphs},
  year      = {2019},
  pages     = {219--234},
  doi       = {10.1007/978-3-030-29249-2_15},
  keywords  = {CBR},
}

@InProceedings{Kirk2007,
  author    = {David Kirk},
  booktitle = {Proceedings of the 6th international symposium on Memory management},
  title     = {{NVIDIA} cuda software and gpu parallel computing architecture},
  year      = {2007},
  month     = {oct},
  publisher = {{ACM}},
  doi       = {10.1145/1296907.1296909},
  keywords  = {Introduction},
}

@Article{BROMLEY1993,
  author    = {Jane Bromley and James W. Bentz and Leon Bottou and Isabelle Guyon and Yann Lecun and Cliff Moore and Eduard Sackinger and Rookpak Shah},
  journal   = {International Journal of Pattern Recognition and Artificial Intelligence},
  title     = {Signature Verification using a "Siamese" Time Delay Neural Network},
  year      = {1993},
  month     = {aug},
  number    = {04},
  pages     = {669--688},
  volume    = {07},
  doi       = {10.1142/s0218001493000339},
  keywords  = {Siamese NN},
  publisher = {World Scientific Pub Co Pte Lt},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 KeywordGroup:CBR\;0\;keywords\;CBR\;0\;0\;1\;0x336633ff\;ALIGN_VERTICAL_BOTTOM\;\;;
1 KeywordGroup:Introduction\;0\;keywords\;Introduction\;0\;0\;1\;0xff0000ff\;NOTEBOOK_PLUS_OUTLINE\;\;;
1 KeywordGroup:Semi-Supervised Learning\;0\;keywords\;Semi-Supervised Learning\;0\;0\;1\;0x000080ff\;AXIS_ARROW\;\;;
1 KeywordGroup:Unsupervised Learning\;0\;keywords\;Unsupervised Learning\;0\;0\;1\;0x008080ff\;AXIS\;\;;
1 KeywordGroup:Supervised Learning\;0\;keywords\;Supervised Learning\;0\;0\;1\;0x00ffffff\;AXIS_ARROW_INFO\;\;;
1 KeywordGroup:Graph Embedding\;0\;keywords\;Graph Embedding\;0\;0\;1\;0xff9966ff\;GRAPHQL\;\;;
1 KeywordGroup:Autoencoder\;0\;keywords\;Autoencoder\;0\;0\;1\;0x800080ff\;ARROW_EXPAND_RIGHT\;\;;
1 KeywordGroup:Semantic Graphs\;0\;keywords\;Semantic Graphs\;0\;0\;1\;0x00ff00ff\;GRAPH_OUTLINE\;\;;
1 KeywordGroup:Siamese NN\;0\;keywords\;Siamese NN\;0\;0\;1\;0xff00ffff\;ARROW_DECISION_AUTO_OUTLINE\;\;;
1 KeywordGroup:Math\;0\;keywords\;Math\;0\;0\;1\;0x666666ff\;MATH_INTEGRAL_BOX\;\;;
1 KeywordGroup:Books\;0\;keywords\;book\;0\;0\;1\;0xcccc33ff\;BOOK\;\;;
1 KeywordGroup:Triplet Loss\;0\;keywords\;Triplet Loss\;0\;0\;1\;0x4d804dff\;\;\;;
1 StaticGroup:Shallow Embedding\;0\;1\;0xff9966ff\;\;\;;
1 StaticGroup:Neural Message Passing\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Data Augmentation\;0\;1\;0x333333ff\;ACCOUNT_CHECK\;\;;
}
